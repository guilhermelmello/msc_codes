Staring Time: Fri Jan  2 01:23:40 -03 2026
Root directory /home/lovelace/proj/proj877/gmello/msc_codes/qwen_pt
Node: adagp05
Sending data to /work/gmello
sending incremental file list
./

sent 214 bytes  received 19 bytes  155.33 bytes/sec
total size is 1,138,625,258  speedup is 4,886,803.68
Starting Causal Language Model Training.
>>> Loading CLM Dataset from Disk
>>> Creating train and test splits for hyperparameter search.
DatasetDict({
    train: Dataset({
        features: ['input_ids', 'attention_mask'],
        num_rows: 199813
    })
    test: Dataset({
        features: ['input_ids', 'attention_mask'],
        num_rows: 22202
    })
})
>>> Loading Tokenizer
PreTrainedTokenizerFast(name_or_path='/home/lovelace/proj/proj877/gmello/msc_codes/tokenizers/bpe-qwen-pt/models/bpe8k-nfc/', vocab_size=8000, model_max_length=2048, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'eos_token': '[EOS]', 'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]'}, clean_up_tokenization_spaces=True, added_tokens_decoder={
	0: AddedToken("[PAD]", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	1: AddedToken("[SEP]", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	2: AddedToken("[EOS]", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	3: AddedToken("[UNK]", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
}
)
>>> Loading Model
OptimizedModule(
  (_orig_mod): Qwen3ForCausalLM(
    (model): Qwen3Model(
      (embed_tokens): Embedding(8000, 1024, padding_idx=0)
      (layers): ModuleList(
        (0-27): 28 x Qwen3DecoderLayer(
          (self_attn): Qwen3Attention(
            (q_proj): Linear(in_features=1024, out_features=2048, bias=False)
            (k_proj): Linear(in_features=1024, out_features=1024, bias=False)
            (v_proj): Linear(in_features=1024, out_features=1024, bias=False)
            (o_proj): Linear(in_features=2048, out_features=1024, bias=False)
            (q_norm): Qwen3RMSNorm((128,), eps=1e-06)
            (k_norm): Qwen3RMSNorm((128,), eps=1e-06)
          )
          (mlp): Qwen3MLP(
            (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)
            (up_proj): Linear(in_features=1024, out_features=3072, bias=False)
            (down_proj): Linear(in_features=3072, out_features=1024, bias=False)
            (act_fn): SiLUActivation()
          )
          (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)
          (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)
        )
      )
      (norm): Qwen3RMSNorm((1024,), eps=1e-06)
      (rotary_emb): Qwen3RotaryEmbedding()
    )
    (lm_head): Linear(in_features=1024, out_features=8000, bias=False)
  )
)
>>> Model Training
{'epoch': 1, 'loss': 2.955041254737931, 'val_loss': 2.4892099693804033}
{'epoch': 2, 'loss': 2.3677786123056754, 'val_loss': 2.339038859870317}
{'epoch': 3, 'loss': 2.2210408724359443, 'val_loss': 2.274338076368876}
{'epoch': 4, 'loss': 2.122324431063759, 'val_loss': 2.242947361311239}
{'epoch': 5, 'loss': 2.0425681186663502, 'val_loss': 2.2274912193804033}
{'epoch': 6, 'loss': 1.9706010604574793, 'val_loss': 2.224091543587896}
{'epoch': 7, 'loss': 1.9009881962604678, 'val_loss': 2.2352080331412103}
{'epoch': 8, 'loss': 1.8396380096471132, 'val_loss': 2.249144452449568}
{'epoch': 9, 'loss': 1.7919210081572376, 'val_loss': 2.268529358789625}
{'epoch': 10, 'loss': 1.7650010679279586, 'val_loss': 2.2789704160662825}
Saving model to ./models/qwen-pt-bpe8k-nfc
Ending Time: Fri Jan  2 22:21:46 -03 2026
