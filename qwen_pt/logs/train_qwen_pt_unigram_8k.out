Staring Time: Thu Jan  1 23:21:53 -03 2026
Root directory /home/lovelace/proj/proj877/gmello/msc_codes/qwen_pt
Node: adagp02
Sending data to /work/gmello
sending incremental file list
./

sent 221 bytes  received 19 bytes  480.00 bytes/sec
total size is 1,481,964,938  speedup is 6,174,853.91
Starting Causal Language Model Training.
>>> Loading CLM Dataset from Disk
>>> Creating train and test splits for hyperparameter search.
DatasetDict({
    train: Dataset({
        features: ['input_ids', 'attention_mask'],
        num_rows: 260068
    })
    test: Dataset({
        features: ['input_ids', 'attention_mask'],
        num_rows: 28897
    })
})
>>> Loading Tokenizer
PreTrainedTokenizerFast(name_or_path='/home/lovelace/proj/proj877/gmello/msc_codes/tokenizers/bpe-qwen-pt/models/unigram8k-nfc/', vocab_size=8000, model_max_length=2048, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'eos_token': '[EOS]', 'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]'}, clean_up_tokenization_spaces=True, added_tokens_decoder={
	0: AddedToken("[PAD]", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	1: AddedToken("[SEP]", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	2: AddedToken("[EOS]", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	3: AddedToken("[UNK]", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
}
)
>>> Loading Model
OptimizedModule(
  (_orig_mod): Qwen3ForCausalLM(
    (model): Qwen3Model(
      (embed_tokens): Embedding(8000, 1024, padding_idx=0)
      (layers): ModuleList(
        (0-27): 28 x Qwen3DecoderLayer(
          (self_attn): Qwen3Attention(
            (q_proj): Linear(in_features=1024, out_features=2048, bias=False)
            (k_proj): Linear(in_features=1024, out_features=1024, bias=False)
            (v_proj): Linear(in_features=1024, out_features=1024, bias=False)
            (o_proj): Linear(in_features=2048, out_features=1024, bias=False)
            (q_norm): Qwen3RMSNorm((128,), eps=1e-06)
            (k_norm): Qwen3RMSNorm((128,), eps=1e-06)
          )
          (mlp): Qwen3MLP(
            (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)
            (up_proj): Linear(in_features=1024, out_features=3072, bias=False)
            (down_proj): Linear(in_features=3072, out_features=1024, bias=False)
            (act_fn): SiLUActivation()
          )
          (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)
          (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)
        )
      )
      (norm): Qwen3RMSNorm((1024,), eps=1e-06)
      (rotary_emb): Qwen3RotaryEmbedding()
    )
    (lm_head): Linear(in_features=1024, out_features=8000, bias=False)
  )
)
>>> Model Training
{'epoch': 1, 'loss': 2.19431862696909, 'val_loss': 1.891446458218041}
{'epoch': 2, 'loss': 1.8035937843305152, 'val_loss': 1.786135514665191}
{'epoch': 3, 'loss': 1.6969594618167485, 'val_loss': 1.738819521306032}
{'epoch': 4, 'loss': 1.6227024464745112, 'val_loss': 1.7138688087991145}
{'epoch': 5, 'loss': 1.5607879538676879, 'val_loss': 1.699657581627006}
{'epoch': 6, 'loss': 1.5032831699105271, 'val_loss': 1.6966657443276147}
{'epoch': 7, 'loss': 1.4465061263391255, 'val_loss': 1.7047160348644161}
{'epoch': 8, 'loss': 1.3969330407039675, 'val_loss': 1.7194590481460985}
{'epoch': 9, 'loss': 1.3586281085667042, 'val_loss': 1.7349673146098505}
{'epoch': 10, 'loss': 1.3381096267289507, 'val_loss': 1.7453436289429995}
Saving model to ./models/qwen-pt-unigram8k-nfc
Ending Time: Sat Jan  3 02:21:12 -03 2026
